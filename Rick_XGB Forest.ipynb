{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required Python Packages\n",
    "import xgboost\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DATA_DIR = '/home/mag/Extra_Disk_1TB/raw_mag_files/2018-03-06/'\n",
    "CHUNK_SIZE = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x Shape ::  (700000, 19)\n",
      "Train_y Shape ::  (700000,)\n",
      "Test_x Shape ::  (300000, 19)\n",
      "Test_y Shape ::  (300000,)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "Train Accuracy ::  0.879077142857\n",
      "Test Accuracy  ::  0.877466666667\n",
      " Confusion matrix  [[ 80468  30201]\n",
      " [  6559 182772]]\n",
      "\n",
      "Feature Importance :: \n",
      "                              importance\n",
      "CON_SERIES_CITE_COUNT          0.101087\n",
      "CON_SERIES_PAPER_COUNT         0.096239\n",
      "FIRST_AUTHOR_CITE_COUNT        0.092125\n",
      "CON_SERIES_RANK                0.075522\n",
      "LAST_AUTHOR_CITE_COUNT         0.072436\n",
      "FIRST_AUTHOR_PAPER_COUNT       0.069791\n",
      "FIRST_AUTHOR_RANK              0.064208\n",
      "LAST_AUTHOR_PAPER_COUNT        0.058184\n",
      "CON_INSTANCE_CITE_COUNT        0.050250\n",
      "FOS_CITE_COUNT_HIGHEST         0.046430\n",
      "LAST_AUTHOR_RANK               0.045842\n",
      "FOS_PAPER_COUNT_HIGHEST        0.043785\n",
      "FOS_ID_HIGHEST                 0.042169\n",
      "CON_INSTANCE_PAPER_COUNT       0.037026\n",
      "LAST_AUTHOR_ORG_RANK           0.034088\n",
      "LAST_AUTHOR_ORG_CITE_COUNT     0.029974\n",
      "LAST_AUTHOR_ORG_PAPER_COUNT    0.025566\n",
      "FOS_LEVEL_HIGHEST              0.014252\n",
      "CON_INSTANCE_RANK              0.001029\n",
      "\n",
      "Classification Report :: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.73      0.81    110669\n",
      "        1.0       0.86      0.97      0.91    189331\n",
      "\n",
      "avg / total       0.88      0.88      0.87    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HEADERS = [\"FIRST_AUTHOR_RANK\", \"FIRST_AUTHOR_PAPER_COUNT\", \"FIRST_AUTHOR_CITE_COUNT\", \"LAST_AUTHOR_RANK\", \"LAST_AUTHOR_PAPER_COUNT\", \"LAST_AUTHOR_CITE_COUNT\", \"LAST_AUTHOR_ORG_RANK\", \"LAST_AUTHOR_ORG_PAPER_COUNT\", \"LAST_AUTHOR_ORG_CITE_COUNT\", \"CON_SERIES_RANK\", \"CON_SERIES_PAPER_COUNT\", \"CON_SERIES_CITE_COUNT\", \"CON_INSTANCE_RANK\", \"CON_INSTANCE_PAPER_COUNT\", \"CON_INSTANCE_CITE_COUNT\", \"FOS_ID_HIGHEST\", \"FOS_LEVEL_HIGHEST\", \"FOS_PAPER_COUNT_HIGHEST\", \"FOS_CITE_COUNT_HIGHEST\", \"AGE\", \"CITED_CLASS\"]\n",
    "\n",
    "def split_dataset(dataset, train_percentage, feature_headers, target_header):\n",
    "    \"\"\"\n",
    "    Split the dataset with train_percentage\n",
    "    :param dataset:\n",
    "    :param train_percentage:\n",
    "    :param feature_headers:\n",
    "    :param target_header:\n",
    "    :return: train_x, test_x, train_y, test_y\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and test dataset\n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset[feature_headers], dataset[target_header],\n",
    "                                                        train_size=train_percentage)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the csv file into pandas dataframe\n",
    "    df_reader = pd.read_csv(DATA_DIR + 'Con_Data_1m_Cited_Sample.txt', dtype='float' , sep='\\t', error_bad_lines=False, chunksize=CHUNK_SIZE)\n",
    "    dataset = pd.concat(df_reader, ignore_index=True)\n",
    "\n",
    "\n",
    "    train_x, test_x, train_y, test_y = split_dataset(dataset, 0.7, HEADERS[0:19], HEADERS[20])\n",
    "\n",
    "    # Train and Test dataset size details\n",
    "    print (\"Train_x Shape :: \", train_x.shape)\n",
    "    print (\"Train_y Shape :: \", train_y.shape)\n",
    "    print (\"Test_x Shape :: \", test_x.shape)\n",
    "    print (\"Test_y Shape :: \", test_y.shape)\n",
    "\n",
    "    \n",
    "    # Create random forest classifier instance\n",
    "    trained_model = XGBClassifier(n_estimators=1000)\n",
    "    trained_model.fit(train_x, train_y)\n",
    "    print(trained_model)\n",
    "    predictions = trained_model.predict(test_x)\n",
    "\n",
    "    # Train and Test Accuracy\n",
    "    print (\"Train Accuracy :: \", accuracy_score(train_y, trained_model.predict(train_x)))\n",
    "    print (\"Test Accuracy  :: \", accuracy_score(test_y, predictions))\n",
    "    print (\" Confusion matrix \", confusion_matrix(test_y, predictions))\n",
    "    \n",
    "    # Printing Feature Importance\n",
    "    print (\"\\nFeature Importance :: \\n\", pd.DataFrame(trained_model.feature_importances_, index = train_x.columns, columns=['importance']).sort_values('importance',ascending=False))\n",
    "    \n",
    "    #Printing Classification Report\n",
    "    print(\"\\nClassification Report :: \\n\", classification_report(test_y, predictions))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x Shape ::  (700000, 20)\n",
      "Train_y Shape ::  (700000,)\n",
      "Test_x Shape ::  (300000, 20)\n",
      "Test_y Shape ::  (300000,)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "Train Accuracy ::  0.738451428571\n",
      "Test Accuracy  ::  0.731936666667\n",
      " Confusion matrix  [[83408 23480  2225  1705]\n",
      " [ 7939 88378  6795  3921]\n",
      " [ 1102 18189 26524  6546]\n",
      " [  278  4319  3920 21271]]\n",
      "\n",
      "Feature Importance :: \n",
      "                              importance\n",
      "CON_SERIES_CITE_COUNT          0.101950\n",
      "FIRST_AUTHOR_CITE_COUNT        0.099713\n",
      "CON_SERIES_PAPER_COUNT         0.090729\n",
      "LAST_AUTHOR_CITE_COUNT         0.085474\n",
      "FIRST_AUTHOR_PAPER_COUNT       0.084318\n",
      "LAST_AUTHOR_PAPER_COUNT        0.073433\n",
      "FIRST_AUTHOR_RANK              0.067507\n",
      "AGE                            0.062400\n",
      "CON_SERIES_RANK                0.061431\n",
      "LAST_AUTHOR_RANK               0.042345\n",
      "CON_INSTANCE_CITE_COUNT        0.035300\n",
      "FOS_ID_HIGHEST                 0.035188\n",
      "FOS_CITE_COUNT_HIGHEST         0.032803\n",
      "FOS_PAPER_COUNT_HIGHEST        0.031386\n",
      "LAST_AUTHOR_ORG_RANK           0.026615\n",
      "LAST_AUTHOR_ORG_CITE_COUNT     0.023782\n",
      "CON_INSTANCE_PAPER_COUNT       0.020763\n",
      "LAST_AUTHOR_ORG_PAPER_COUNT    0.018116\n",
      "FOS_LEVEL_HIGHEST              0.006449\n",
      "CON_INSTANCE_RANK              0.000298\n",
      "\n",
      "Classification Report :: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.90      0.75      0.82    110818\n",
      "        2.0       0.66      0.83      0.73    107033\n",
      "        3.0       0.67      0.51      0.58     52361\n",
      "        4.0       0.64      0.71      0.67     29788\n",
      "\n",
      "avg / total       0.75      0.73      0.73    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HEADERS = [\"FIRST_AUTHOR_RANK\", \"FIRST_AUTHOR_PAPER_COUNT\", \"FIRST_AUTHOR_CITE_COUNT\", \"LAST_AUTHOR_RANK\", \"LAST_AUTHOR_PAPER_COUNT\", \"LAST_AUTHOR_CITE_COUNT\", \"LAST_AUTHOR_ORG_RANK\", \"LAST_AUTHOR_ORG_PAPER_COUNT\", \"LAST_AUTHOR_ORG_CITE_COUNT\", \"CON_SERIES_RANK\", \"CON_SERIES_PAPER_COUNT\", \"CON_SERIES_CITE_COUNT\", \"CON_INSTANCE_RANK\", \"CON_INSTANCE_PAPER_COUNT\", \"CON_INSTANCE_CITE_COUNT\", \"FOS_ID_HIGHEST\", \"FOS_LEVEL_HIGHEST\", \"FOS_PAPER_COUNT_HIGHEST\", \"FOS_CITE_COUNT_HIGHEST\", \"AGE\", \"CITED_CLASS\", \"CITE_CLASS_SCORE\"]\n",
    "\n",
    "def split_dataset(dataset, train_percentage, feature_headers, target_header):\n",
    "    \"\"\"\n",
    "    Split the dataset with train_percentage\n",
    "    :param dataset:\n",
    "    :param train_percentage:\n",
    "    :param feature_headers:\n",
    "    :param target_header:\n",
    "    :return: train_x, test_x, train_y, test_y\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and test dataset\n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset[feature_headers], dataset[target_header],\n",
    "                                                        train_size=train_percentage)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the csv file into pandas dataframe\n",
    "    df_reader = pd.read_csv(DATA_DIR + 'Con_Data_1m_Cited_ZScore.txt', dtype='float' , sep='\\t', error_bad_lines=False, chunksize=CHUNK_SIZE)\n",
    "    dataset = pd.concat(df_reader, ignore_index=True)\n",
    "\n",
    "\n",
    "    train_x, test_x, train_y, test_y = split_dataset(dataset, 0.7, HEADERS[0:20], HEADERS[21])\n",
    "\n",
    "    # Train and Test dataset size details\n",
    "    print (\"Train_x Shape :: \", train_x.shape)\n",
    "    print (\"Train_y Shape :: \", train_y.shape)\n",
    "    print (\"Test_x Shape :: \", test_x.shape)\n",
    "    print (\"Test_y Shape :: \", test_y.shape)\n",
    "\n",
    "    \n",
    "    # Create random forest classifier instance\n",
    "    trained_model = XGBClassifier(n_estimators=1000)\n",
    "    trained_model.fit(train_x, train_y)\n",
    "    print(trained_model)\n",
    "    predictions = trained_model.predict(test_x)\n",
    "\n",
    "    # Train and Test Accuracy\n",
    "    print (\"Train Accuracy :: \", accuracy_score(train_y, trained_model.predict(train_x)))\n",
    "    print (\"Test Accuracy  :: \", accuracy_score(test_y, predictions))\n",
    "    print (\" Confusion matrix \", confusion_matrix(test_y, predictions))\n",
    "    \n",
    "    # Printing Feature Importance\n",
    "    print (\"\\nFeature Importance :: \\n\", pd.DataFrame(trained_model.feature_importances_, index = train_x.columns, columns=['importance']).sort_values('importance',ascending=False))\n",
    "    \n",
    "    #Printing Classification Report\n",
    "    print(\"\\nClassification Report :: \\n\", classification_report(test_y, predictions))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x Shape ::  (700000, 20)\n",
      "Train_y Shape ::  (700000,)\n",
      "Test_x Shape ::  (300000, 20)\n",
      "Test_y Shape ::  (300000,)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "Train Accuracy ::  0.713591428571\n",
      "Test Accuracy  ::  0.709293333333\n",
      " Confusion matrix  [[91281  4644  8639  6993]\n",
      " [11583 32889 10558  7679]\n",
      " [ 7147  5987 35091 14619]\n",
      " [ 2493   151  6719 53527]]\n",
      "\n",
      "Feature Importance :: \n",
      "                              importance\n",
      "FIRST_AUTHOR_CITE_COUNT        0.106385\n",
      "CON_SERIES_CITE_COUNT          0.091906\n",
      "LAST_AUTHOR_CITE_COUNT         0.088806\n",
      "CON_SERIES_PAPER_COUNT         0.084420\n",
      "FIRST_AUTHOR_PAPER_COUNT       0.079959\n",
      "LAST_AUTHOR_PAPER_COUNT        0.073381\n",
      "FIRST_AUTHOR_RANK              0.065139\n",
      "AGE                            0.059166\n",
      "CON_SERIES_RANK                0.059015\n",
      "LAST_AUTHOR_RANK               0.043250\n",
      "FOS_CITE_COUNT_HIGHEST         0.037768\n",
      "FOS_ID_HIGHEST                 0.037541\n",
      "CON_INSTANCE_CITE_COUNT        0.033950\n",
      "FOS_PAPER_COUNT_HIGHEST        0.032173\n",
      "LAST_AUTHOR_ORG_RANK           0.029980\n",
      "LAST_AUTHOR_ORG_CITE_COUNT     0.025746\n",
      "CON_INSTANCE_PAPER_COUNT       0.022268\n",
      "LAST_AUTHOR_ORG_PAPER_COUNT    0.020037\n",
      "FOS_LEVEL_HIGHEST              0.009111\n",
      "CON_INSTANCE_RANK              0.000000\n",
      "\n",
      "Classification Report :: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.81      0.82      0.81    111557\n",
      "        2.0       0.75      0.52      0.62     62709\n",
      "        3.0       0.58      0.56      0.57     62844\n",
      "        4.0       0.65      0.85      0.73     62890\n",
      "\n",
      "avg / total       0.72      0.71      0.70    300000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HEADERS = [\"FIRST_AUTHOR_RANK\", \"FIRST_AUTHOR_PAPER_COUNT\", \"FIRST_AUTHOR_CITE_COUNT\", \"LAST_AUTHOR_RANK\", \"LAST_AUTHOR_PAPER_COUNT\", \"LAST_AUTHOR_CITE_COUNT\", \"LAST_AUTHOR_ORG_RANK\", \"LAST_AUTHOR_ORG_PAPER_COUNT\", \"LAST_AUTHOR_ORG_CITE_COUNT\", \"CON_SERIES_RANK\", \"CON_SERIES_PAPER_COUNT\", \"CON_SERIES_CITE_COUNT\", \"CON_INSTANCE_RANK\", \"CON_INSTANCE_PAPER_COUNT\", \"CON_INSTANCE_CITE_COUNT\", \"FOS_ID_HIGHEST\", \"FOS_LEVEL_HIGHEST\", \"FOS_PAPER_COUNT_HIGHEST\", \"FOS_CITE_COUNT_HIGHEST\", \"AGE\", \"CITED_CLASS\", \"CITE_CLASS_SCORE\"]\n",
    "\n",
    "def split_dataset(dataset, train_percentage, feature_headers, target_header):\n",
    "    \"\"\"\n",
    "    Split the dataset with train_percentage\n",
    "    :param dataset:\n",
    "    :param train_percentage:\n",
    "    :param feature_headers:\n",
    "    :param target_header:\n",
    "    :return: train_x, test_x, train_y, test_y\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataset into train and test dataset\n",
    "    train_x, test_x, train_y, test_y = train_test_split(dataset[feature_headers], dataset[target_header],\n",
    "                                                        train_size=train_percentage)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load the csv file into pandas dataframe\n",
    "    df_reader = pd.read_csv(DATA_DIR + 'Con_Data_1m_Score_Sample.txt', dtype='float' , sep='\\t', error_bad_lines=False, chunksize=CHUNK_SIZE)\n",
    "    dataset = pd.concat(df_reader, ignore_index=True)\n",
    "\n",
    "\n",
    "    train_x, test_x, train_y, test_y = split_dataset(dataset, 0.7, HEADERS[0:20], HEADERS[21])\n",
    "\n",
    "    # Train and Test dataset size details\n",
    "    print (\"Train_x Shape :: \", train_x.shape)\n",
    "    print (\"Train_y Shape :: \", train_y.shape) \n",
    "    print (\"Test_x Shape :: \", test_x.shape)\n",
    "    print (\"Test_y Shape :: \", test_y.shape)\n",
    "\n",
    "    \n",
    "    # Create random forest classifier instance\n",
    "    trained_model = XGBClassifier(n_estimators=1000)\n",
    "    trained_model.fit(train_x, train_y)\n",
    "    print(trained_model)\n",
    "    predictions = trained_model.predict(test_x)\n",
    "\n",
    "    # Train and Test Accuracy\n",
    "    print (\"Train Accuracy :: \", accuracy_score(train_y, trained_model.predict(train_x)))\n",
    "    print (\"Test Accuracy  :: \", accuracy_score(test_y, predictions))\n",
    "    print (\" Confusion matrix \", confusion_matrix(test_y, predictions))\n",
    "    \n",
    "    # Printing Feature Importance\n",
    "    print (\"\\nFeature Importance :: \\n\", pd.DataFrame(trained_model.feature_importances_, index = train_x.columns, columns=['importance']).sort_values('importance',ascending=False))\n",
    "    \n",
    "    #Printing Classification Report\n",
    "    print(\"\\nClassification Report :: \\n\", classification_report(test_y, predictions))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
